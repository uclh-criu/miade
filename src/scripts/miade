#!/usr/bin/env python
import os
from pathlib import Path
from shutil import rmtree
from typing import Optional

import typer
import yaml
import pandas as pd
from medcat.cat import CAT
from medcat.config import Config
from pydantic import BaseModel

from miade.model_builders import CDBBuilder
from miade.model_builders import VocabBuilder

app = typer.Typer()


class CLI_Config(BaseModel):
    snomed_data_path: Optional[Path] = None
    fdb_data_path: Optional[Path] = None
    elg_data_path: Optional[Path] = None
    snomed_subset_path: Optional[Path] = None
    snomed_exclusions_path: Optional[Path] = None
    medcat_config_file: Optional[Path] = None
    training_data_path: Path
    output_dir: Path

    @classmethod
    def from_yaml_file(cls, config_filepath: Path):
        with config_filepath.open("r") as stream:
            config_dict = yaml.safe_load(stream)
            return cls(**config_dict)


@app.command()
def build_model_pack(
        config_file: Path,
        temp: Optional[Path] = typer.Argument(Path.cwd() / Path(".temp"))
):
    config = CLI_Config.from_yaml_file(config_file)

    # Load MedCAT configuration
    medcat_config = Config()
    if config.medcat_config_file:
        medcat_config.parse_config_file(str(config.medcat_config_file))

    cdb_builder = CDBBuilder(
        temp_dir=temp,
        snomed_data_path=config.snomed_data_path,
        fdb_data_path=config.fdb_data_path,
        elg_data_path=config.elg_data_path,
        snomed_subset_path=config.snomed_subset_path,
        snomed_exclusions_path=config.snomed_exclusions_path
    )

    cdb_builder.preprocess()
    cdb = cdb_builder.create_cdb()
    vocab_builder = VocabBuilder()

    with open(
            config.training_data_path, "r", encoding="utf-8"
    ) as training_data:
        training_data_list = [line.strip() for line in training_data]

    vocab = vocab_builder.create_new_vocab(
        training_data_list=training_data_list,
        cdb=cdb,
        config=medcat_config,
        output_dir=config.output_dir,
    )

    cat = CAT(cdb=cdb, config=cdb.config, vocab=vocab)
    cat.create_model_pack(str(config.output_dir))


@app.command()
def train(model: Path, data: Path,
          checkpoint: int = 5000,
          train_partial: Optional[int] = None,
          output: Optional[Path] = typer.Argument(Path.cwd())):

    if data.suffix == ".csv":
        print("Loading csv file...")
        df = pd.read_csv(data)
        training_data = df.source.to_list()
        if train_partial:
            print(f"partial training {train_partial} documents")
            training_data = training_data[:train_partial]
    else:
        with data.open('r') as d:
            training_data = [line.strip() for line in d]
    print("training data length: ", len(training_data))

    cat = CAT.load_model_pack(str(model))
    if checkpoint:
        cat.config.general["checkpoint"]["steps"] = checkpoint
        cat.config.general["checkpoint"]["output_dir"] = os.path.join(Path.cwd(), "checkpoints")

    cat.train(training_data)
    cat.create_model_pack(str(output))


@app.command()
def train_supervised(model: Path, annotations_path: Path,
                     nepochs: int,
                     use_filters=True,
                     print_stats=True,
                     output: Optional[Path] = typer.Argument(Path.cwd())):

    cat = CAT.load_model_pack(str(model))

    cat.train_supervised(data_path=str(annotations_path),
                         nepochs=nepochs,
                         use_filters=use_filters,
                         print_stats=print_stats)

    cat.create_model_pack(str(output))


if __name__ == "__main__":
    app()
